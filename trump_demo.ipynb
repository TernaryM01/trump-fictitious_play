{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd88221",
   "metadata": {},
   "source": [
    "## Training Trump players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d814424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"true\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.86\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51a5113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " ...\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]]\n"
     ]
    }
   ],
   "source": [
    "# Some actual computation to wake up that lazy ass\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.ones((10000, 10000))\n",
    "y = jnp.dot(x, x)\n",
    "\n",
    "print(y)  # Make sure computation actually happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185975c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9561e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trump_utils import *\n",
    "from cfvfp_main import DeepCFVFPSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DeepCFVFPSolver(\n",
    "    game_name=\"trump\",\n",
    "    q_value_network_models=q_value_network_models,\n",
    "    avg_policy_network_models=avg_policy_network_models,\n",
    "    info_state_tensor_transformers=info_state_tensor_transformers,\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    dummy_infostate=np.array([dummy_infostate], dtype=np.float32),\n",
    "    data_augmentors=data_augmentors,\n",
    "    revelation_transformer=revelation_transformer,\n",
    "    revelation_intensity=[0.1, 0.1],\n",
    "    revelation_decay_mode='linear',\n",
    "    num_iterations=150,\n",
    "    num_iterations_q_per_pi=3,\n",
    "    num_traversals_per_player=72,\n",
    "    uniform=True,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size_q_value=[512, 128, 2048], \n",
    "    batch_size_avg_policy=[512, 128, 1024], \n",
    "    q_value_network_train_steps=[500, 250, 1000],\n",
    "    avg_policy_network_train_steps=[250, 250, 500],\n",
    "    q_value_memory_capacity=[6e3, 1e3, 7.8e4],\n",
    "    avg_policy_memory_capacity=[3e4, 5e3, 3.9e5],\n",
    "    save_dir_buffers=\"cfvfp_buffers_z3-q-only\",\n",
    "    save_dir_nets=\"cfvfp_nets_z3-q-only\",\n",
    "    seed=2,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "# 5. Run the training loop\n",
    "print(f\"\\n--- Starting Trump game training with DeepCFVFPSolver ({solver._num_phases} phases) ---\")\n",
    "print(f\"Global number of actions used by solver: {solver._global_num_actions}\")\n",
    "\n",
    "final_policy_params_by_phase, q_losses_by_phase, avg_policy_losses_by_phase = solver.solve()\n",
    "\n",
    "print(\"--- Training finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616499e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = DeepCFVFPSolver(\n",
    "    game_name=\"trump\",\n",
    "    q_value_network_models=q_value_network_models,\n",
    "    avg_policy_network_models=avg_policy_network_models,\n",
    "    info_state_tensor_transformers=[bid_transformer, ad_transformer, play_transformer_pi],\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    dummy_infostate=np.array([dummy_infostate], dtype=np.float32),\n",
    "    data_augmentors=data_augmentors,\n",
    "    revelation_transformer=revelation_transformer,\n",
    "    revelation_intensity=[0.2, 0.2],\n",
    "    revelation_decay_mode='linear',\n",
    "    num_iterations=600,\n",
    "    num_iterations_q_per_pi=6,\n",
    "    num_traversals_per_player=300,\n",
    "    uniform=True,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size_q_value=[2048, 512, 4096], \n",
    "    batch_size_avg_policy=[2048, 1024, 2048],\n",
    "    q_value_network_train_steps=[1500, 120, 3000],\n",
    "    avg_policy_network_train_steps=[500, 30, 800], \n",
    "    q_value_memory_capacity=[6e4, 2e4, 7.8e5],\n",
    "    avg_policy_memory_capacity=[6e4, 2e4, 7.8e5],\n",
    "    save_dir_buffers=\"cfvfp_buffers\",\n",
    "    save_dir_nets=\"cfvfp_nets\",\n",
    "    seed=1,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "# 5. Run the training loop\n",
    "print(f\"\\n--- Starting Trump game training with DeepCFVFPSolver ({solver._num_phases} phases) ---\")\n",
    "print(f\"Global number of actions used by solver: {solver._global_num_actions}\")\n",
    "\n",
    "final_policy_params_by_phase, q_losses_by_phase, avg_policy_losses_by_phase = solver.solve()\n",
    "\n",
    "print(\"--- Training finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "582aea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Value Losses (last value per player, per phase index):\n",
      "  Phase 'bid':\n",
      "    Player 0: 17.6334 (Avg: 16.9620, Min: 16.2329, Max: 17.8518 over 30 entries)\n",
      "  Phase 'ad':\n",
      "    Player 0: 0.6131 (Avg: 0.5561, Min: 0.2232, Max: 1.0016 over 30 entries)\n",
      "  Phase 'play':\n",
      "    Player 0: 2.8631 (Avg: 2.7940, Min: 2.4560, Max: 2.9394 over 30 entries)\n",
      "\n",
      "Average Policy Losses (last value per player, per phase index):\n",
      "  Phase 'bid':\n",
      "    Player 0: 0.0379 (Avg: 0.0270, Min: 0.0140, Max: 0.0379 over 10 entries)\n",
      "  Phase 'ad':\n",
      "    Player 0: 0.0053 (Avg: 0.0064, Min: 0.0008, Max: 0.0134 over 10 entries)\n",
      "  Phase 'play':\n",
      "    Player 0: 0.8269 (Avg: 0.8239, Min: 0.8185, Max: 0.8269 over 10 entries)\n",
      "\n",
      "Solver run complete.\n"
     ]
    }
   ],
   "source": [
    "# 6. Optional: Print or analyze the losses\n",
    "print(\"\\nQ-Value Losses (last value per player, per phase index):\")\n",
    "for phase, p_losses_dict in q_losses_by_phase.items():\n",
    "    # Assuming phase_idx corresponds to the integer index (0, 1, 2)\n",
    "    phase_name = PHASES[phase] # Use PHASES list to get the name\n",
    "    print(f\"  Phase '{phase_name}':\")\n",
    "    for player, losses_list in p_losses_dict.items():\n",
    "        if losses_list:\n",
    "            print(f\"    Player {player}: {losses_list[-1]:.4f} (Avg: {np.mean(losses_list):.4f}, Min: {np.min(losses_list):.4f}, Max: {np.max(losses_list):.4f} over {len(losses_list)} entries)\")\n",
    "        else:\n",
    "            print(f\"    Player {player}: No Q-loss data\")\n",
    "\n",
    "print(\"\\nAverage Policy Losses (last value per player, per phase index):\")\n",
    "for phase, p_losses_dict in avg_policy_losses_by_phase.items():\n",
    "    phase_name = PHASES[phase]\n",
    "    print(f\"  Phase '{phase_name}':\")\n",
    "    for player, losses_list in p_losses_dict.items():\n",
    "        if losses_list:\n",
    "            print(f\"    Player {player}: {losses_list[-1]:.4f} (Avg: {np.mean(losses_list):.4f}, Min: {np.min(losses_list):.4f}, Max: {np.max(losses_list):.4f} over {len(losses_list)} entries)\")\n",
    "        else:\n",
    "            print(f\"    Player {player}: No AvgPolicy-loss data\")\n",
    "\n",
    "print(\"\\nSolver run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58219e46",
   "metadata": {},
   "source": [
    "## Interactive Game with trained strategy displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c417c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Short Name: trump\n",
      "Game Long Name: Trump\n",
      "Number of Players: 4\n",
      "Min Utility: -33.0\n",
      "Max Utility: 28.0\n",
      "Max Game Length: 120\n",
      "Tensor Shape (Infostate): [588]\n"
     ]
    }
   ],
   "source": [
    "import pyspiel\n",
    "\n",
    "game = pyspiel.load_game(\"trump\")\n",
    "\n",
    "def print_game_info(game):\n",
    "    game_type = game.get_type()\n",
    "    print(\"Game Short Name:\", game_type.short_name)\n",
    "    print(\"Game Long Name:\", game_type.long_name)\n",
    "    print(\"Number of Players:\", game.num_players())\n",
    "    print(\"Min Utility:\", game.min_utility())\n",
    "    print(\"Max Utility:\", game.max_utility())\n",
    "    print(\"Max Game Length:\", game.max_game_length())\n",
    "    print(\"Tensor Shape (Infostate):\", game.information_state_tensor_shape())\n",
    "    \n",
    "print_game_info(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bc31e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING INTERACTIVE TRUMP GAME ===\n",
      "\n",
      "--- 1. Dealing Cards (Randomly) ---\n",
      "\n",
      "--- Initial Hands Dealt (Player 0's perspective for their hand) ---\n",
      "Player 0 Hand: C3, C5, CT, D4, D6, D7, D8, DA, HK, S2, S5, S7, ST\n",
      "Player 1 Hand: C2, C7, CK, DK, H4, H5, H6, H7, H9, HT, S3, SJ, SK\n",
      "Player 2 Hand: C6, C8, CA, D2, D3, H2, H3, H8, HA, HQ, S6, S8, S9\n",
      "Player 3 Hand: C4, C9, CJ, CQ, D5, D9, DJ, DQ, DT, HJ, S4, SA, SQ\n",
      "\n",
      "--- 2. Bidding Phase ---\n",
      "Current state before bids:\n",
      "Phase: Bidding\n",
      "Current Player: P0\n",
      "Hands (visible in full state string):\n",
      "P0: C3 C5 CT DA D4 D6 D7 D8 HK S2 S5 S7 ST\n",
      "P1: C2 C7 CK DK H4 H5 H6 H7 H9 HT S3 SJ SK\n",
      "P2: CA C6 C8 D2 D3 HA H2 H3 H8 HQ S6 S8 S9\n",
      "P3: C4 C9 CJ CQ D5 D9 DT DJ DQ HJ SA S4 SQ\n",
      "Bid Cards Status: (All bids are hidden until revealed simultaneously after this phase)\n",
      "\n",
      "\n",
      "Player 0's turn to bid.\n",
      "  Legal moves for P0: C5:0.01 D4:0.03 HK:0.00 ST:0.00 S5:0.00 D6:0.02 D8:0.00 S2:0.00 C3:0.94 CT:0.00 D7:0.00 DA:0.00 S7:0.00\n",
      "  A random choice: C3\n",
      "Player 0 bids with: C3\n",
      "\n",
      "Player 1's turn to bid.\n",
      "  Legal moves for P1: C7:0.00 DK:0.00 H6:0.00 SJ:0.00 CK:0.00 HT:0.00 H4:0.57 S3:0.15 SK:0.00 H7:0.03 H9:0.00 H5:0.26 C2:0.00\n",
      "  A random choice: H4\n",
      "Player 1 bids with: H4\n",
      "\n",
      "Player 2's turn to bid.\n",
      "  Legal moves for P2: CA:0.01 D2:0.00 H3:0.01 S6:0.00 S9:0.00 D3:0.00 C6:0.00 HQ:0.00 S8:0.00 C8:0.00 HA:0.00 H2:0.96 H8:0.00\n",
      "  A random choice: H2\n",
      "Player 2 bids with: H2\n",
      "\n",
      "Player 3's turn to bid.\n",
      "  Legal moves for P3: C4:0.22 C9:0.00 CJ:0.00 CQ:0.00 D5:0.00 D9:0.01 DT:0.00 DJ:0.00 DQ:0.10 HJ:0.00 SA:0.04 S4:0.62 SQ:0.00\n",
      "  A random choice: S4\n",
      "Player 3 bids with: S4\n",
      "\n",
      "--- Bid Reveal and Round Determination ---\n",
      "Phase: HighLowDecision\n",
      "Current Player: P3\n",
      "Hands (visible in full state string):\n",
      "P0: C3 C5 CT DA D4 D6 D7 D8 HK S2 S5 S7 ST\n",
      "P1: C2 C7 CK DK H4 H5 H6 H7 H9 HT S3 SJ SK\n",
      "P2: CA C6 C8 D2 D3 HA H2 H3 H8 HQ S6 S8 S9\n",
      "P3: C4 C9 CJ CQ D5 D9 DT DJ DQ HJ SA S4 SQ\n",
      "Revealed Bid Cards: P0:C3 P1:H4 P2:H2 P3:S4 \n",
      "Round Determination: Error: Bid Status 0 Post-Bidding\n",
      "Resulting Round Type (for scoring): Undecided\n",
      "Trump suit: S\n",
      "Internal Target Bid values (for scoring logic): P0:3 P1:4 P2:2 P3:4 \n",
      "Highest bidder (determined trump): P3\n",
      "\n",
      "3. Trump Suit: S\n",
      "\n",
      "--- 4. High/Low Decision Phase ---\n",
      "Player 3 (Highest Bidder) must choose to Ascend or Descend.\n",
      "  Choices: Ascend:0.26 Descend:0.74\n",
      "  A random choice: Ascend\n",
      "Player 3 chose: Ascend\n",
      "State after High/Low decision:\n",
      "Phase: Play\n",
      "Current Player: P3\n",
      "Hands (visible in full state string):\n",
      "P0: C3 C5 CT DA D4 D6 D7 D8 HK S2 S5 S7 ST\n",
      "P1: C2 C7 CK DK H4 H5 H6 H7 H9 HT S3 SJ SK\n",
      "P2: CA C6 C8 D2 D3 HA H2 H3 H8 HQ S6 S8 S9\n",
      "P3: C4 C9 CJ CQ D5 D9 DT DJ DQ HJ SA S4 SQ\n",
      "Revealed Bid Cards: P0:C3 P1:H4 P2:H2 P3:S4 \n",
      "Round Determination: Ascend Occurred\n",
      "Resulting Round Type (for scoring): High\n",
      "Trump suit: S\n",
      "Internal Target Bid values (for scoring logic): P0:4 P1:5 P2:3 P3:5 \n",
      "Highest bidder (determined trump): P3\n",
      "Trick #1\n",
      "Current trick (Leader P3): \n",
      "Trump break occurred (from previous tricks): No\n",
      "Tricks won: P0:0 P1:0 P2:0 P3:0 \n",
      "\n",
      "5. Final Round Type (for scoring): High (based on: Ascend Occurred)\n",
      "\n",
      "--- 6. Trick Play Phase ---\n",
      "\n",
      "-- Trick 1 --\n",
      "  Tricks collected so far: P0:0, P1:0, P2:0, P3:0\n",
      "  Bid Cards: P0:C3 P1:H4 P2:H2 P3:S4\n",
      "  Broken: False\n",
      "  Trump Suit: S\n",
      "  Remaining Hands AT START of Trick 1:\n",
      "    P0: C3, C5, CT, D4, D6, D7, D8, DA, HK, S2, S5, S7, ST\n",
      "    P1: C2, C7, CK, DK, H4, H5, H6, H7, H9, HT, S3, SJ, SK\n",
      "    P2: C6, C8, CA, D2, D3, H2, H3, H8, HA, HQ, S6, S8, S9\n",
      "    P3: C4, C9, CJ, CQ, D5, D9, DJ, DQ, DT, HJ, S4, SA, SQ\n",
      "  Led by: P3\n",
      "\n",
      "  Player 3's turn.\n",
      "  Legal moves for P3: C4:0.09:1 C9:0.12:3 CJ:0.11:3 CQ:0.09:3 D5:0.10:3 D9:0.08:3 DT:0.10:3 DJ:0.07:3 DQ:0.17:3 HJ:0.08:3\n",
      "  A random choice: DT\n",
      "   -> P3 plays: DT\n",
      "\n",
      "  Player 0's turn.\n",
      "  Legal moves for P0: D4:0.17:1 D6:0.18:1 D8:0.20:1 D7:0.20:1 DA:0.25:4\n",
      "  A random choice: D7\n",
      "   -> P0 plays: D7\n",
      "\n",
      "  Player 1's turn.\n",
      "  Legal moves for P1: DK:1.00:3\n",
      "  A random choice: DK\n",
      "  Only one legal move: DK (auto-played)\n",
      "   -> P1 plays: DK\n",
      "\n",
      "  Player 2's turn.\n",
      "  Legal moves for P2: D2:0.26:1 D3:0.74:1\n",
      "  A random choice: D3\n",
      "   -> P2 plays: D3\n",
      "  Cards played this trick (P0-P3 order): D7, DK, D3, DT\n",
      "\n",
      "-- Trick 2 --\n",
      "  Tricks collected so far: P0:0, P1:1, P2:0, P3:0\n",
      "  Bid Cards: P0:C3 P1:H4 P2:H2 P3:S4\n",
      "  Broken: False\n",
      "  Trump Suit: S\n",
      "  Remaining Hands AT START of Trick 2:\n",
      "    P0: C3, C5, CT, D4, D6, D8, DA, HK, S2, S5, S7, ST\n",
      "    P1: C2, C7, CK, H4, H5, H6, H7, H9, HT, S3, SJ, SK\n",
      "    P2: C6, C8, CA, D2, H2, H3, H8, HA, HQ, S6, S8, S9\n",
      "    P3: C4, C9, CJ, CQ, D5, D9, DJ, DQ, HJ, S4, SA, SQ\n",
      "  Led by: P1\n",
      "\n",
      "  Player 1's turn.\n",
      "  Legal moves for P1: C7:0.10:3 H6:0.10:1 CK:0.13:3 HT:0.06:3 H4:0.09:1 H7:0.14:1 H9:0.18:3 H5:0.09:1 C2:0.11:1\n",
      "  A random choice: H4\n",
      "   -> P1 plays: H4\n",
      "\n",
      "  Player 2's turn.\n",
      "  Legal moves for P2: H3:0.21:1 HQ:0.13:3 HA:0.22:4 H2:0.32:1 H8:0.12:3\n",
      "  A random choice: HA\n",
      "   -> P2 plays: HA\n",
      "\n",
      "  Player 3's turn.\n",
      "  Legal moves for P3: HJ:1.00:1\n",
      "  A random choice: HJ\n",
      "  Only one legal move: HJ (auto-played)\n",
      "   -> P3 plays: HJ\n",
      "\n",
      "  Player 0's turn.\n",
      "  Legal moves for P0: HK:1.00:1\n",
      "  A random choice: HK\n",
      "  Only one legal move: HK (auto-played)\n",
      "   -> P0 plays: HK\n",
      "  Cards played this trick (P0-P3 order): HK, H4, HA, HJ\n",
      "\n",
      "-- Trick 3 --\n",
      "  Tricks collected so far: P0:0, P1:1, P2:1, P3:0\n",
      "  Bid Cards: P0:C3 P1:H4 P2:H2 P3:S4\n",
      "  Broken: False\n",
      "  Trump Suit: S\n",
      "  Remaining Hands AT START of Trick 3:\n",
      "    P0: C3, C5, CT, D4, D6, D8, DA, S2, S5, S7, ST\n",
      "    P1: C2, C7, CK, H5, H6, H7, H9, HT, S3, SJ, SK\n",
      "    P2: C6, C8, CA, D2, H2, H3, H8, HQ, S6, S8, S9\n",
      "    P3: C4, C9, CJ, CQ, D5, D9, DJ, DQ, S4, SA, SQ\n",
      "  Led by: P2\n",
      "\n",
      "  Player 2's turn.\n",
      "  Legal moves for P2: CA:0.27:4 D2:0.05:1 H3:0.11:1 C6:0.10:3 HQ:0.09:4 C8:0.14:3 H2:0.16:1 H8:0.06:3\n",
      "  A random choice: C8\n",
      "Game quit by user during card play.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "from trump_interactive import interactive_trump_game\n",
    "\n",
    "state = interactive_trump_game(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8904b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 0 C3: 0 C4: 0 C5: 0 C6: 2 C7: 0 C8: 2 C9: 0 CT: 0 CJ: 0 CQ: 0 CK: 0 CA: 2 \n",
      "D: D2: 2 D3: 0 D4: 0 D5:-1 D6: 2 D7: 0 D8: 2 D9: 0 DT: 0 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3: 2 H4: 2 H5: 2 H6: 2 H7: 0 H8: 0 H9: 0 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 2 S3: 2 S4: 2 S5: 0 S6: 1 S7: 2 S8: 0 S9: 0 ST: 2 SJ: 2 SQ: 0 SK: 0 SA: 1 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 0 C3: 0 C4: 0 C5: 0 C6: 2 C7: 0 C8: 2 C9: 0 CT: 0 CJ: 0 CQ: 0 CK: 0 CA: 2 \n",
      "D: D2: 2 D3: 0 D4: 0 D5: 2 D6: 2 D7: 0 D8: 2 D9: 0 DT: 0 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3: 2 H4: 2 H5: 2 H6: 2 H7: 0 H8: 0 H9: 0 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 2 S3: 2 S4: 1 S5: 0 S6: 2 S7: 2 S8: 0 S9: 0 ST: 1 SJ: 2 SQ: 0 SK: 0 SA: 2 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 0 C3: 0 C4: 0 C5: 0 C6: 2 C7: 0 C8: 2 C9: 0 CT: 0 CJ: 0 CQ: 0 CK: 0 CA: 2 \n",
      "D: D2: 2 D3: 0 D4: 0 D5: 2 D6:-1 D7: 0 D8: 2 D9: 0 DT: 0 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3: 2 H4: 2 H5: 2 H6: 2 H7: 0 H8: 0 H9: 0 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 2 S3: 1 S4: 2 S5: 0 S6: 2 S7: 2 S8: 0 S9: 0 ST: 2 SJ: 1 SQ: 0 SK: 0 SA: 2 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=1 Unknown=31, Had=2, Never Had=18\n",
      "Opponent 2: Has=0 Unknown=31, Had=2, Never Had=19\n",
      "Opponent 3: Has=1 Unknown=31, Had=2, Never Had=18\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "tensor = state.information_state_tensor(state.current_player())\n",
    "print_graveyards(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe21259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 2.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, -1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 12.0, 1.0, 0.0, 0.0, 0.0, 13.0, 1.0, 0.0, 0.0, 0.0, 11.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 1.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, -1.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, -1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, -1.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 4.0, 2.0, 2.0, 3.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 7.0, 0.0, 0.0, 0.0, 1.0, 6.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 1 C3: 2 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9:-1 CT:-1 CJ: 2 CQ: 2 CK:-1 CA:-1 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 2 D6: 1 D7: 1 D8: 2 D9:-1 DT: 2 DJ: 2 DQ: 2 DK: 1 DA: 2 \n",
      "H: H2: 2 H3:-1 H4: 2 H5: 2 H6: 2 H7: 2 H8: 2 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 1 S4: 2 S5: 2 S6: 2 S7: 2 S8: 2 S9: 2 ST: 2 SJ: 1 SQ:-1 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 2 C3: 1 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9: 2 CT: 2 CJ:-1 CQ: 2 CK: 2 CA: 2 \n",
      "D: D2: 1 D3: 2 D4:-1 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT: 1 DJ: 2 DQ: 2 DK: 2 DA: 1 \n",
      "H: H2:-1 H3: 2 H4: 2 H5: 2 H6:-1 H7:-1 H8: 2 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 1 S3: 2 S4:-1 S5: 2 S6: 2 S7: 1 S8: 2 S9: 2 ST:-1 SJ: 2 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 2 C3: 2 C4:-1 C5:-1 C6: 1 C7: 2 C8: 2 C9: 2 CT: 2 CJ: 2 CQ:-1 CK: 2 CA: 2 \n",
      "D: D2: 2 D3: 1 D4: 2 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT: 2 DJ: 1 DQ: 1 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5:-1 H6: 2 H7: 2 H8: 2 H9: 2 HT: 2 HJ: 2 HQ:-1 HK: 2 HA:-1 \n",
      "S: S2: 2 S3: 2 S4: 2 S5: 1 S6: 2 S7: 2 S8: 2 S9: 2 ST: 2 SJ: 2 SQ: 2 SK: 1 SA:-1 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=7 Unknown=0, Had=6, Never Had=39\n",
      "Opponent 2: Has=7 Unknown=0, Had=6, Never Had=39\n",
      "Opponent 3: Has=7 Unknown=0, Had=6, Never Had=39\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import revelation_transformer\n",
    "\n",
    "tensor0 = state.information_state_tensor(0)\n",
    "tensor1 = state.information_state_tensor(1)\n",
    "tensor2 = state.information_state_tensor(2)\n",
    "tensor3 = state.information_state_tensor(3)\n",
    "all_tensors = [tensor0, tensor1, tensor2, tensor3]\n",
    "\n",
    "tensor_revealed = revelation_transformer(all_tensors, state.current_player())\n",
    "print_graveyards(tensor_revealed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d9331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 11, 37, 5, 40, 12, 9, 16, 50, 15, 32, 47, 14, 28, 10, 24, 43, 26, 17, 2, 19, 48, 4, 35, 18, 0, 3, 29, 25, 1, 30, 41, 34, 44, 7, 46, 27, 8, 20, 33, 51, 42, 39, 49, 21, 6, 22, 23, 45, 31, 36, 13, 40, 28, 27, 42, 13, 16, 24, 25, 23, 15, 19, 20, 21, 14, 18, 8, 39, 50, 48, 41, 51, 40, 43, 44, 22, 5, 17, 34, 45, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 3246, in _complete\n",
      "    result = matcher(context)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 2139, in magic_matcher\n",
      "    matches = self.magic_matches(text)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 2172, in magic_matches\n",
      "    global_matches = self.global_matches(bare_text)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 1114, in global_matches\n",
      "    for word in lst:\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    }
   ],
   "source": [
    "print(state.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e77a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3♣ loses due to not following suit.\n",
      "6♣ loses due to not following suit.\n",
      "Q♦ loses due to not following suit.\n",
      "K♦ loses due to not following suit.\n",
      "A valid play where player plays 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: J♦ 8♥ 4♠ J♠ Q♠ A♠\n",
      "  P2: 3♦ 8♦ T♦ Q♥ 7♠ 9♠\n",
      "  P3: 5♦ 7♦ 2♥ 3♥ 9♥ K♠\n",
      "Current Trick:    P1:8♥ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 7♠ 9♠ Q♠ A♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ K♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ 4♠ J♠\n",
      "Current Trick:    P1:7♠ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ Q♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ K♠\n",
      "Current Trick:    P1:4♠ P2:Q♥ P3:9♥\n",
      "A valid play where player plays T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ Q♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ K♠\n",
      "Current Trick:    P1:4♠ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ Q♠ K♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ 9♠ J♠\n",
      "Current Trick:    P1:K♠ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ K♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ Q♠\n",
      "Current Trick:    P1:9♠ P2:Q♥ P3:9♥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trump_z3_bool_current_hand import analyze_hand_cards\n",
    "import numpy as np\n",
    "\n",
    "tensor = np.array(state.information_state_tensor(state.current_player()))\n",
    "analyze_hand_cards(tensor, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3♣ loses to 6♣\n",
      "5♣ loses to 6♣\n",
      "A valid play where player plays T♣:\n",
      "Opponent Current Hands:\n",
      "  P1: 4♣ 2♦ 5♦ 6♦ 7♦ 9♦ T♥ 2♠ 3♠ 4♠ T♠\n",
      "  P2: 9♣ 4♦ Q♦ K♦ 7♥ K♥ 5♠ 8♠ 9♠ Q♠ K♠\n",
      "  P3: 6♣ Q♣ A♣ 3♦ J♦ 5♥ 6♥ 8♥ J♥ J♠ A♠\n",
      "Current Trick:    P1:4♣ P2:9♣ P3:6♣\n",
      "A valid play where player plays K♣:\n",
      "Opponent Current Hands:\n",
      "  P1: 4♣ 2♦ 5♦ 6♦ 7♦ 9♦ T♥ 2♠ 3♠ 4♠ T♠\n",
      "  P2: 9♣ 4♦ Q♦ K♦ 7♥ K♥ 5♠ 8♠ 9♠ Q♠ K♠\n",
      "  P3: 6♣ Q♣ A♣ 3♦ J♦ 5♥ 6♥ 8♥ J♥ J♠ A♠\n",
      "Current Trick:    P1:4♣ P2:9♣ P3:6♣\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 4,\n",
       " -1,\n",
       " -1,\n",
       " 4,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trump_utils import reveal_p0\n",
    "from trump_z3_bool_current_hand import analyze_hand_cards\n",
    "\n",
    "tensor_p1 = np.array(state.information_state_tensor(0))\n",
    "tensor_p2 = np.array(state.information_state_tensor(1))\n",
    "tensor_p3 = np.array(state.information_state_tensor(2))\n",
    "\n",
    "tensor_all = np.array([tensor, tensor_p1, tensor_p2, tensor_p3])\n",
    "\n",
    "tensor_revealed = reveal_p0(tensor_all)\n",
    "analyze_hand_cards(tensor_revealed, debug=True, max_timeout_ms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090101d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2:-1 C3: 2 C4: 2 C5: 2 C6: 2 C7:-1 C8: 2 C9: 2 CT: 2 CJ: 2 CQ: 2 CK:-1 CA: 2 \n",
      "D: D2:-1 D3: 2 D4: 2 D5: 1 D6: 2 D7: 2 D8: 2 D9: 2 DT: 2 DJ: 2 DQ:-1 DK: 2 DA:-1 \n",
      "H: H2: 2 H3:-1 H4: 2 H5: 2 H6: 2 H7:-1 H8:-1 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 2 S4: 2 S5:-1 S6: 2 S7:-1 S8: 2 S9: 2 ST: 2 SJ:-1 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 2 C3: 2 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9:-1 CT: 2 CJ:-1 CQ: 2 CK: 2 CA: 2 \n",
      "D: D2: 2 D3: 2 D4: 1 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT:-1 DJ: 2 DQ: 2 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5:-1 H6:-1 H7: 2 H8: 2 H9: 2 HT: 2 HJ:-1 HQ:-1 HK: 2 HA: 2 \n",
      "S: S2:-1 S3:-1 S4: 2 S5: 2 S6:-1 S7: 2 S8:-1 S9: 2 ST:-1 SJ: 2 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 2 C3: 2 C4:-1 C5:-1 C6: 2 C7: 2 C8: 2 C9: 2 CT:-1 CJ: 2 CQ:-1 CK: 2 CA:-1 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 2 D6: 2 D7: 1 D8:-1 D9:-1 DT: 2 DJ: 2 DQ: 2 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5: 2 H6: 2 H7: 2 H8: 2 H9:-1 HT:-1 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 2 S4: 2 S5: 2 S6: 2 S7: 2 S8: 2 S9:-1 ST: 2 SJ: 2 SQ: 2 SK:-1 SA:-1 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=12 Unknown=0, Had=1, Never Had=39\n",
      "Opponent 2: Has=12 Unknown=0, Had=1, Never Had=39\n",
      "Opponent 3: Has=12 Unknown=0, Had=1, Never Had=39\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "print_graveyards(tensor_revealed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39146a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q♣ loses due to not following suit.\n",
      "K♣ loses due to not following suit.\n",
      "3♠ loses due to not following suit.\n",
      "6♠ loses due to not following suit.\n",
      "A valid play where player plays 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: J♣ J♦ Q♦ 2♥ 3♥ 8♥\n",
      "  P2: 5♣ 7♣ T♣ 4♦ 7♦ Q♥\n",
      "  P3: 3♣ 8♣ 9♦ K♦ A♦ 9♥\n",
      "Current Trick:    P1:3♥ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 7♦ Q♦ K♦ A♦\n",
      "  P2: 5♣ 9♦ J♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ T♣ J♣ 4♦ 2♥ 9♥\n",
      "Current Trick:    P1:7♦ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ 7♦ K♦ A♦\n",
      "  P2: 9♦ J♦ Q♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:4♦ P2:Q♥ P3:9♥\n",
      "A valid play where player plays T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ 7♦ K♦ A♦\n",
      "  P2: 9♦ J♦ Q♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:4♦ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ Q♦ K♦ A♦\n",
      "  P2: 7♦ 9♦ J♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:Q♦ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 8♣ 4♦ 7♦ Q♦ K♦ A♦\n",
      "  P2: 9♦ J♦ 2♥ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ 7♣ T♣ J♣ 9♥\n",
      "Current Trick:    P1:7♦ P2:Q♥ P3:9♥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trump_utils import data_augmentor\n",
    "import jax.numpy as jnp\n",
    "\n",
    "tensor_permuted = data_augmentor(jnp.asarray(tensor), jax.random.PRNGKey(22))\n",
    "analyze_hand_cards(tensor_permuted, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005da7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 0 D6: 1 D7: 2 D8: 0 D9: 0 DT: 2 DJ: 2 DQ: 0 DK: 0 DA: 1 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5:-1 S6: 2 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 2 D4: 1 D5: 0 D6: 2 D7: 2 D8: 0 D9: 0 DT: 1 DJ: 2 DQ: 0 DK: 0 DA: 2 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5: 2 S6: 2 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 1 D4: 2 D5: 0 D6: 2 D7: 2 D8: 0 D9: 0 DT: 2 DJ: 1 DQ: 0 DK: 0 DA: 2 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5: 2 S6:-1 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=1 Unknown=31, Had=2, Never Had=18\n",
      "Opponent 2: Has=0 Unknown=31, Had=2, Never Had=19\n",
      "Opponent 3: Has=1 Unknown=31, Had=2, Never Had=18\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "tensor = state.information_state_tensor(state.current_player())\n",
    "print_graveyards(np.array(tensor_permuted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, -1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 6.0, 8.0, 6.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93259c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f5df7",
   "metadata": {},
   "source": [
    "## Players from different iterations playing against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48551e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import pyspiel\n",
    "from typing import List, Dict, Any, Callable\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_params(save_dir_nets, player, phase, iteration):\n",
    "    \"\"\"Load policy parameters for a player, phase, and iteration.\"\"\"\n",
    "    pi_params_path = os.path.join(save_dir_nets, f\"player{player}\", f\"phase{phase}\", \"pi_data\", f\"pi_params_iter{iteration}.msgpack\")\n",
    "    if not os.path.exists(pi_params_path):\n",
    "        raise FileNotFoundError(f\"Parameter file not found: {pi_params_path}\")\n",
    "    with open(pi_params_path, 'rb') as f:\n",
    "        state_dict = flax.serialization.from_bytes(None, f.read())\n",
    "    if 'params' not in state_dict:\n",
    "        raise KeyError(f\"'params' key not found in loaded state_dict from {pi_params_path}\")\n",
    "    return state_dict['params']\n",
    "\n",
    "def evaluate_players(\n",
    "    game_name: str,\n",
    "    save_dir_nets: str,\n",
    "    iterations_per_player: List[int],\n",
    "    num_eval_games: int,\n",
    "    pi_models: List[nn.Module],\n",
    "    info_state_tensor_transformers: List[Callable[[jax.Array], jax.Array]],\n",
    "    action_transformers: List[Callable[[jax.Array], jax.Array]],\n",
    "    phase_classifier_fn: Callable[[jax.Array], int],\n",
    "    uniform: bool = False,\n",
    "    seed: int = 42\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Evaluate players by having them play against each other.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Name of the game (e.g., 'kuhn_poker')\n",
    "        save_dir_nets: Directory where network parameters are saved\n",
    "        iterations_per_player: List of iteration numbers for each player [player0_iter, player1_iter, ...]\n",
    "        num_eval_games: Number of games to play for evaluation\n",
    "        pi_models: List of policy network models for each phase\n",
    "        info_state_tensor_transformers: List of transformers for each phase\n",
    "        action_transformers: List of action transformers for each phase\n",
    "        phase_classifier_fn: Function to classify game phase from info state\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player_id to average score across all games\n",
    "    \"\"\"\n",
    "    # Initialize game and random state\n",
    "    game = pyspiel.load_game(game_name)\n",
    "    num_players = game.num_players()\n",
    "    global_num_actions = game.num_distinct_actions()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    num_phases = len(pi_models)\n",
    "    \n",
    "    if len(iterations_per_player) != num_players:\n",
    "        raise ValueError(f\"Number of iterations ({len(iterations_per_player)}) must match number of players ({num_players})\")\n",
    "    \n",
    "    # Load all player parameters for all phases\n",
    "    player_params = []\n",
    "    for player in range(num_players):\n",
    "        player_phase_params = []\n",
    "        for phase in range(num_phases):\n",
    "            params = load_params(save_dir_nets, player if not uniform else 0, phase, iterations_per_player[player])\n",
    "            player_phase_params.append(params)\n",
    "        player_params.append(player_phase_params)\n",
    "    \n",
    "    # Setup JIT compiled inference functions for all players and phases\n",
    "    jitted_inference_pi = []\n",
    "    for phase in range(num_phases):\n",
    "        phase_inferences = []\n",
    "        for player in range(num_players if not uniform else 1):\n",
    "            inference_fn = _get_jitted_avg_policy(pi_models[phase], action_transformers[phase])\n",
    "            phase_inferences.append(inference_fn)\n",
    "        jitted_inference_pi.append(phase_inferences)\n",
    "    \n",
    "    # Play evaluation games\n",
    "    total_scores = defaultdict(float)\n",
    "    \n",
    "    for _game_round in tqdm(range(num_eval_games)):\n",
    "        state = game.new_initial_state()\n",
    "        \n",
    "        while not state.is_terminal():\n",
    "            if state.is_chance_node():\n",
    "                # Handle chance nodes\n",
    "                chance_outcome_actions, chance_outcome_probs = zip(*state.chance_outcomes())\n",
    "                chance_outcome_probs_np = np.array(chance_outcome_probs, dtype=np.float64)\n",
    "                chance_outcome_probs_np /= np.sum(chance_outcome_probs_np)\n",
    "                sampled_action = np.random.choice(chance_outcome_actions, p=chance_outcome_probs_np)\n",
    "                state = state.child(sampled_action)\n",
    "            else:\n",
    "                # Handle player decision nodes\n",
    "                active_player = state.current_player()\n",
    "                \n",
    "                # Get info state and determine phase\n",
    "                full_info_state_np = np.array(state.information_state_tensor(active_player), dtype=np.float32)\n",
    "                phase = phase_classifier_fn(full_info_state_np)\n",
    "                \n",
    "                # Transform info state for this phase\n",
    "                info_state_transformed_np = info_state_tensor_transformers[phase](full_info_state_np)\n",
    "                legal_actions_mask_global_np = np.array(state.legal_actions_mask(active_player), dtype=bool)\n",
    "                \n",
    "                # Add batch dimensions for network inference\n",
    "                info_state_transformed_np = jnp.expand_dims(info_state_transformed_np, axis=0)\n",
    "                legal_actions_mask_global_np = jnp.expand_dims(legal_actions_mask_global_np, axis=0)\n",
    "                \n",
    "                # Get policy probabilities from the network\n",
    "                policy_probs_global = jitted_inference_pi[phase][active_player if not uniform else 0](\n",
    "                    player_params[active_player if not uniform else 0][phase],\n",
    "                    info_state_transformed_np,\n",
    "                    legal_actions_mask_global_np\n",
    "                )\n",
    "                \n",
    "                # Convert to numpy and ensure proper normalization\n",
    "                policy_probs_np_global = np.array(policy_probs_global)\n",
    "                policy_probs_np_global /= np.sum(policy_probs_np_global)\n",
    "                \n",
    "                # Sample action according to policy\n",
    "                sampled_action = np.random.choice(global_num_actions, p=policy_probs_np_global)\n",
    "                state = state.child(sampled_action)\n",
    "        \n",
    "        # Return final scores/returns for all players\n",
    "        returns = state.returns()\n",
    "        game_scores = {player: returns[player] for player in range(len(returns))}\n",
    "        \n",
    "        # Accumulate scores\n",
    "        for player, score in game_scores.items():\n",
    "            total_scores[player] += score\n",
    "    \n",
    "    # Calculate average scores\n",
    "    average_scores = {player_idx: total_scores[player_idx] / num_eval_games \n",
    "                     for player_idx in range(num_players)}\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "def _get_jitted_avg_policy(pi_model_instance, action_transformer_fn):\n",
    "    \"\"\"Create JIT compiled policy inference function.\"\"\"\n",
    "    @jax.jit\n",
    "    def get_policy(params_avg_policy: Any, info_state_transformed: jax.Array, legal_actions_mask_global: jax.Array):\n",
    "        masked_logits_net_output = pi_model_instance.apply(\n",
    "            {'params': params_avg_policy}, info_state_transformed, legal_actions_mask_global\n",
    "        )\n",
    "        masked_logits_global = action_transformer_fn(masked_logits_net_output)\n",
    "        avg_policy_probs_global = jax.nn.softmax(masked_logits_global, axis=-1)\n",
    "        return jnp.squeeze(avg_policy_probs_global, axis=0)\n",
    "    return get_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863927a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808b163ff72b400fb5fac983461b596f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m\n\u001b[1;32m     47\u001b[0m info_state_tensor_transformers \u001b[38;5;241m=\u001b[39m [bid_transformer, ad_transformer, play_transformer_pi]\n\u001b[1;32m     49\u001b[0m pi_models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpBiddingPolicyNet()),  \u001b[38;5;66;03m# Phase 0\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpAD_PolicyNet()),      \u001b[38;5;66;03m# Phase 1\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpPlayPolicyNet())      \u001b[38;5;66;03m# Phase 2\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ]\n\u001b[0;32m---> 55\u001b[0m \u001b[43mevaluate_players\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgame_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir_nets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcfvfp_nets_z3-q-only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations_per_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m700\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muniform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_eval_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpi_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo_state_tensor_transformers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo_state_tensor_transformers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_transformers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_transformers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_classifier_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrump_phase_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 124\u001b[0m, in \u001b[0;36mevaluate_players\u001b[0;34m(game_name, save_dir_nets, iterations_per_player, num_eval_games, pi_models, info_state_tensor_transformers, action_transformers, phase_classifier_fn, uniform, seed)\u001b[0m\n\u001b[1;32m    121\u001b[0m         policy_probs_np_global \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(policy_probs_np_global)\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# Sample action according to policy\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m         sampled_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(global_num_actions, p\u001b[38;5;241m=\u001b[39mpolicy_probs_np_global)\n\u001b[1;32m    125\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mchild(sampled_action)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Return final scores/returns for all players\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trump_utils import (\n",
    "    PolicyNetworkWrapper,\n",
    "    TrumpBiddingPolicyNet, TrumpAD_PolicyNet, TrumpPlayPolicyNet,\n",
    "    action_transformers, #info_state_tensor_transformers,\n",
    "    trump_phase_classifier\n",
    ")\n",
    "\n",
    "# --- Tensor Specification, Slicing Helper, and Phase Detection Indices ---\n",
    "TENSOR_COMPONENT_SPEC = [\n",
    "    (\"Hand\", 52), \n",
    "    (\"BidCards\", 4 * 5),       # 20\n",
    "    (\"TrumpSuit\", 4),          \n",
    "    (\"RoundBidStatus\", 1),     \n",
    "    (\"History\", 13 * (4 + 4 * 5)),   # 312\n",
    "    (\"OpponentGraveyard\", 3 * 52), # 156\n",
    "    (\"ANTC\", 4),               \n",
    "    (\"BreakOccurred\", 1),      \n",
    "    (\"CurrentTrickCards\", 4 * 5), # 20\n",
    "    (\"CurrentTrickLeader\", 4), \n",
    "    (\"CurrentTrickTrumpUncertainty\", 13),    # NEW ITEM #11\n",
    "    (\"CurrentTrickNumber\", 1)  \n",
    "]\n",
    "\n",
    "GLOBAL_NUM_ACTIONS = 52\n",
    "\n",
    "_tensor_component_slices: Dict[str, slice] = {}\n",
    "_current_offset = 0\n",
    "for _name, _size in TENSOR_COMPONENT_SPEC:\n",
    "    _tensor_component_slices[_name] = slice(_current_offset, _current_offset + _size)\n",
    "    _current_offset += _size\n",
    "\n",
    "# Define indices for phase classification using the slices\n",
    "INDEX_ROUND_BID_STATUS = _tensor_component_slices[\"RoundBidStatus\"].start\n",
    "INDEX_FIRST_BID_CARD_FEATURE = _tensor_component_slices[\"BidCards\"].start\n",
    "INDEX_HAND_START = _tensor_component_slices[\"Hand\"].start\n",
    "INDEX_HAND_END = _tensor_component_slices[\"Hand\"].stop\n",
    "INDEX_BID_CARDS_END = _tensor_component_slices[\"BidCards\"].stop\n",
    "\n",
    "def bid_transformer(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor[..., INDEX_HAND_START:INDEX_HAND_END]\n",
    "\n",
    "def ad_transformer(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor[..., INDEX_HAND_START:INDEX_BID_CARDS_END]\n",
    "\n",
    "def play_transformer_pi(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor\n",
    "info_state_tensor_transformers = [bid_transformer, ad_transformer, play_transformer_pi]\n",
    "\n",
    "pi_models = [\n",
    "    PolicyNetworkWrapper(phase_net=TrumpBiddingPolicyNet()),  # Phase 0\n",
    "    PolicyNetworkWrapper(phase_net=TrumpAD_PolicyNet()),      # Phase 1\n",
    "    PolicyNetworkWrapper(phase_net=TrumpPlayPolicyNet())      # Phase 2\n",
    "]\n",
    "\n",
    "evaluate_players(\n",
    "    game_name=\"trump\",\n",
    "    save_dir_nets=\"cfvfp_nets_z3-q-only\",\n",
    "    iterations_per_player=[300, 300, 300, 700],\n",
    "    uniform=True,\n",
    "    num_eval_games=1000,\n",
    "    pi_models=pi_models,\n",
    "    info_state_tensor_transformers=info_state_tensor_transformers,\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    seed=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f469c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
