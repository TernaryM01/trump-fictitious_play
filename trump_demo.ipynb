{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd88221",
   "metadata": {},
   "source": [
    "## Training Trump players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d814424",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"true\"\n",
    "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"0.86\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e51a5113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " ...\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]\n",
      " [10000. 10000. 10000. ... 10000. 10000. 10000.]]\n"
     ]
    }
   ],
   "source": [
    "# Some actual computation to wake up that lazy ass\n",
    "\n",
    "import jax.numpy as jnp\n",
    "\n",
    "x = jnp.ones((10000, 10000))\n",
    "y = jnp.dot(x, x)\n",
    "\n",
    "print(y)  # Make sure computation actually happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "185975c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9561e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from trump_utils import *\n",
    "from nfsp_main import NFSPSolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = NFSPSolver(\n",
    "    game_name=\"trump\",\n",
    "    q_value_network_models=q_value_network_models,\n",
    "    avg_policy_network_models=avg_policy_network_models,\n",
    "    info_state_tensor_transformers=info_state_tensor_transformers,\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    dummy_infostate=np.array([dummy_infostate], dtype=np.float32),\n",
    "    data_augmentors=data_augmentors,\n",
    "    revelation_transformer=revelation_transformer,\n",
    "    revelation_intensity=[1.0, 0.5],\n",
    "    revelation_decay_mode='linear',\n",
    "    num_iterations=600,\n",
    "    num_iterations_q_per_pi=26,\n",
    "    pi_traversals_multiplier=5,\n",
    "    num_traversals_per_player=12,\n",
    "    uniform=True,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size_q_value=[512, 128, 2048], \n",
    "    batch_size_avg_policy=[512, 128, 1024], \n",
    "    q_value_network_train_steps=[50, 25, 100],\n",
    "    avg_policy_network_train_steps=[25, 25, 50],\n",
    "    q_value_memory_capacity=[6e3, 1e3, 7.8e4],\n",
    "    avg_policy_memory_capacity=[3e4, 5e3, 3.9e5],\n",
    "    save_dir_buffers=\"nfsp_buffers_z3-q-only\",\n",
    "    save_dir_nets=\"nfsp_nets_z3-q-only\",\n",
    "    seed=2,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "# 5. Run the training loop\n",
    "print(f\"\\n--- Starting Trump game training with NFSPSolver ({solver._num_phases} phases) ---\")\n",
    "print(f\"Global number of actions used by solver: {solver._global_num_actions}\")\n",
    "\n",
    "final_policy_params_by_phase, q_losses_by_phase, avg_policy_losses_by_phase = solver.solve()\n",
    "\n",
    "print(\"--- Training finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616499e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Trump game training with NFSPSolver (3 phases) ---\n",
      "Global number of actions used by solver: 52\n",
      "Resuming from iteration 420, starting at iteration 421\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae6578e791104089ad662494242ea0a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Overall Iteration:   0%|          | 0/180 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e6d84bb6474ba0af85226a83805f59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b400d47c4f8a4e0581765d6ff4574e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "391af25db3a5479882d257fbb1c3aee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a685ed930bb1414aa57d6b388e982d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9d812adaab4395a7ee6e49de695b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0072340cebba40eb91115e18d47b5fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "384937162792413d909586fdabbae58d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72324d9f88784f01b916acf2c25f9072",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e0a92aa4a254fd2aa56b60079c5b8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "425001c5a8804d53ace54305ab1c80e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08ed6a1b965745938cd11f8c77a3d40f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39581584bb2b41da80b21c5436e5fe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1af793e3e024707a7222899695702ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae1ec354fdd40149c83d01905257385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f76bdf06e50e472388ed3c2922e2b321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de08006764d04aca875905bb03a329e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d662138d1ff94e7995ae1240d94dc939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9acfa83177f4472a07f5b157b90eaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3052472731246b08755c3b8018f8f7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9c24c1159d4fa0b7249234589795b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a6c0a86567c444596451c4700051800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4371c5dcef1f44d692b6ce79391af1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0d2cd6f1c04fff8cc3b1c2a0b6e205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5855cedf53044071a880fb4fecace5b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c9482b69f343508a2f5eecc2ba0309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fca213a7757e40a18fde215368d90f0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff7f54ba19b748eabe3e811644f52ce8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2f4ee334784d86b513b426ca84cb55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e661990c02400e8c2a9d264ca738af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c25c53ddc44046a483349d38bd5584ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0064980f008f4ffe9e117d2de944823e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971ce57416304b48b1ced799615acb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d40e891ded034f9ea8e3ba5386b37718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6929c86f364d4ceaa94881cfc0ac305d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1283b85c1818465b9ffc11421e2f94b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dba8c61b70a49cea8a2cc411e160f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd831ffc31e64f35ad780cc4a6dd0899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34dcf44c1eb447ab3a48157617827e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3569f6a1b04226b6237add1f29fbba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39900ce9025748c888738d28b7634cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d39e12f09ae0403e8f096da6c67383e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faae0715c6744b79a0295b331c44fce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804217a93628430e9e9d72548dbe81ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00692cff53c407ab5f4832d1f6f03a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa34ab042f3b47719c82ee96572c8123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Phase:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0592179af13044fbb7d17230fbeb5f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea393ec82d24ecca2da09c6c07d4ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QNet Train:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "solver = NFSPSolver(\n",
    "    game_name=\"trump\",\n",
    "    q_value_network_models=q_value_network_models,\n",
    "    avg_policy_network_models=avg_policy_network_models,\n",
    "    info_state_tensor_transformers=[bid_transformer, ad_transformer, play_transformer_pi],\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    dummy_infostate=np.array([dummy_infostate], dtype=np.float32),\n",
    "    data_augmentors=data_augmentors,\n",
    "    revelation_transformer=revelation_transformer,\n",
    "    revelation_intensity=[1.0, 0.0],\n",
    "    revelation_decay_mode='linear',\n",
    "    num_iterations=600,\n",
    "    num_iterations_q_per_pi=30,\n",
    "    pi_traversals_multiplier=10,\n",
    "    num_traversals_per_player=30,\n",
    "    horizon=3,\n",
    "    uniform=True,\n",
    "    learning_rate=1e-4,\n",
    "    batch_size_q_value=[2048, 256, 4096], \n",
    "    batch_size_avg_policy=[2048, 768, 3072],\n",
    "    q_value_network_train_steps=[150, 25, 500],\n",
    "    avg_policy_network_train_steps=[500, 30, 1500],\n",
    "    q_value_memory_capacity=[6e4, 2e4, 7.8e5],\n",
    "    avg_policy_memory_capacity=[6e4, 2e4, 7.8e5],\n",
    "    save_dir_buffers=\"nfsp_buffers\",\n",
    "    save_dir_nets=\"nfsp_nets\",\n",
    "    seed=1,\n",
    "    num_workers=6\n",
    ")\n",
    "\n",
    "# 5. Run the training loop\n",
    "print(f\"\\n--- Starting Trump game training with NFSPSolver ({solver._num_phases} phases) ---\")\n",
    "print(f\"Global number of actions used by solver: {solver._global_num_actions}\")\n",
    "\n",
    "final_policy_params_by_phase, q_losses_by_phase, avg_policy_losses_by_phase = solver.solve()\n",
    "\n",
    "print(\"--- Training finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "582aea31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Value Losses (last value per player, per phase index):\n",
      "  Phase 'bid':\n",
      "    Player 0: 2.4424 (Avg: 2.8855, Min: 2.4347, Max: 3.3456 over 118 entries)\n",
      "  Phase 'ad':\n",
      "    Player 0: 1.5774 (Avg: 1.7593, Min: 1.5774, Max: 1.9565 over 118 entries)\n",
      "  Phase 'play':\n",
      "    Player 0: 1.8946 (Avg: 1.9035, Min: 1.8720, Max: 1.9410 over 118 entries)\n",
      "\n",
      "Average Policy Losses (last value per player, per phase index):\n",
      "  Phase 'bid':\n",
      "    Player 0: 0.0903 (Avg: 0.0865, Min: 0.0827, Max: 0.0903 over 2 entries)\n",
      "  Phase 'ad':\n",
      "    Player 0: 0.2309 (Avg: 0.2345, Min: 0.2309, Max: 0.2381 over 2 entries)\n",
      "  Phase 'play':\n",
      "    Player 0: 0.7997 (Avg: 0.8009, Min: 0.7997, Max: 0.8021 over 2 entries)\n",
      "\n",
      "Solver run complete.\n"
     ]
    }
   ],
   "source": [
    "# 6. Optional: Print or analyze the losses\n",
    "print(\"\\nQ-Value Losses (last value per player, per phase index):\")\n",
    "for phase, p_losses_dict in q_losses_by_phase.items():\n",
    "    # Assuming phase_idx corresponds to the integer index (0, 1, 2)\n",
    "    phase_name = PHASES[phase] # Use PHASES list to get the name\n",
    "    print(f\"  Phase '{phase_name}':\")\n",
    "    for player, losses_list in p_losses_dict.items():\n",
    "        if losses_list:\n",
    "            print(f\"    Player {player}: {losses_list[-1]:.4f} (Avg: {np.mean(losses_list):.4f}, Min: {np.min(losses_list):.4f}, Max: {np.max(losses_list):.4f} over {len(losses_list)} entries)\")\n",
    "        else:\n",
    "            print(f\"    Player {player}: No Q-loss data\")\n",
    "\n",
    "print(\"\\nAverage Policy Losses (last value per player, per phase index):\")\n",
    "for phase, p_losses_dict in avg_policy_losses_by_phase.items():\n",
    "    phase_name = PHASES[phase]\n",
    "    print(f\"  Phase '{phase_name}':\")\n",
    "    for player, losses_list in p_losses_dict.items():\n",
    "        if losses_list:\n",
    "            print(f\"    Player {player}: {losses_list[-1]:.4f} (Avg: {np.mean(losses_list):.4f}, Min: {np.min(losses_list):.4f}, Max: {np.max(losses_list):.4f} over {len(losses_list)} entries)\")\n",
    "        else:\n",
    "            print(f\"    Player {player}: No AvgPolicy-loss data\")\n",
    "\n",
    "print(\"\\nSolver run complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58219e46",
   "metadata": {},
   "source": [
    "## Interactive Game with trained strategy displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c417c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Short Name: trump\n",
      "Game Long Name: Trump\n",
      "Number of Players: 4\n",
      "Min Utility: -33.0\n",
      "Max Utility: 28.0\n",
      "Max Game Length: 120\n",
      "Tensor Shape (Infostate): [588]\n"
     ]
    }
   ],
   "source": [
    "import pyspiel\n",
    "\n",
    "game = pyspiel.load_game(\"trump\")\n",
    "\n",
    "def print_game_info(game):\n",
    "    game_type = game.get_type()\n",
    "    print(\"Game Short Name:\", game_type.short_name)\n",
    "    print(\"Game Long Name:\", game_type.long_name)\n",
    "    print(\"Number of Players:\", game.num_players())\n",
    "    print(\"Min Utility:\", game.min_utility())\n",
    "    print(\"Max Utility:\", game.max_utility())\n",
    "    print(\"Max Game Length:\", game.max_game_length())\n",
    "    print(\"Tensor Shape (Infostate):\", game.information_state_tensor_shape())\n",
    "    \n",
    "print_game_info(game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc31e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STARTING INTERACTIVE TRUMP GAME ===\n",
      "\n",
      "--- 1. Dealing Cards (Randomly) ---\n",
      "\n",
      "--- Initial Hands Dealt (Player 0's perspective for their hand) ---\n",
      "Player 0 Hand: C2, C8, D3, D4, D5, D7, DQ, H3, H5, H9, HQ, S6, SQ\n",
      "Player 1 Hand: C3, C7, CT, D2, D9, DK, H4, H6, H8, S3, S8, S9, SA\n",
      "Player 2 Hand: C4, C6, CA, CK, D6, DT, H7, HA, HK, HT, S2, S7, SK\n",
      "Player 3 Hand: C5, C9, CJ, CQ, D8, DA, DJ, H2, HJ, S4, S5, SJ, ST\n",
      "\n",
      "--- 2. Bidding Phase ---\n",
      "Current state before bids:\n",
      "Phase: Bidding\n",
      "Current Player: P0\n",
      "Hands (visible in full state string):\n",
      "P0: C2 C8 D3 D4 D5 D7 DQ H3 H5 H9 HQ S6 SQ\n",
      "P1: C3 C7 CT D2 D9 DK H4 H6 H8 SA S3 S8 S9\n",
      "P2: CA C4 C6 CK D6 DT HA H7 HT HK S2 S7 SK\n",
      "P3: C5 C9 CJ CQ DA D8 DJ H2 HJ S4 S5 ST SJ\n",
      "Bid Cards Status: (All bids are hidden until revealed simultaneously after this phase)\n",
      "\n",
      "\n",
      "Player 0's turn to bid.\n",
      "  Legal moves for P0: C2:0.10 D5:0.06 HQ:0.07 SQ:0.10 D3:0.14 D7:0.11 S6:0.12 H5:0.02 H3:0.07 D4:0.04 DQ:0.05 H9:0.05 C8:0.08\n",
      "  A random choice: C2\n",
      "Player 0 bids with: D3\n",
      "\n",
      "Player 1's turn to bid.\n",
      "  Legal moves for P1: C3:0.16 D2:0.09 H8:0.04 S3:0.08 H4:0.06 S8:0.10 SA:0.07 H6:0.08 DK:0.09 C7:0.06 CT:0.05 D9:0.06 S9:0.06\n",
      "  A random choice: SA\n",
      "Player 1 bids with: C3\n",
      "\n",
      "Player 2's turn to bid.\n",
      "  Legal moves for P2: CA:0.09 DT:0.10 HA:0.05 S2:0.15 HK:0.07 C4:0.04 C6:0.03 HT:0.03 S7:0.12 SK:0.09 CK:0.04 D6:0.02 H7:0.17\n",
      "  A random choice: S2\n",
      "Player 2 bids with: H7\n",
      "\n",
      "Player 3's turn to bid.\n",
      "  Legal moves for P3: C5:0.09 C9:0.17 CJ:0.11 CQ:0.07 DA:0.10 D8:0.04 DJ:0.04 H2:0.14 HJ:0.05 S4:0.05 S5:0.05 ST:0.06 SJ:0.03\n",
      "  A random choice: CJ\n",
      "Player 3 bids with: H2\n",
      "\n",
      "--- Bid Reveal and Round Determination ---\n",
      "Phase: Play\n",
      "Current Player: P2\n",
      "Hands (visible in full state string):\n",
      "P0: C2 C8 D3 D4 D5 D7 DQ H3 H5 H9 HQ S6 SQ\n",
      "P1: C3 C7 CT D2 D9 DK H4 H6 H8 SA S3 S8 S9\n",
      "P2: CA C4 C6 CK D6 DT HA H7 HT HK S2 S7 SK\n",
      "P3: C5 C9 CJ CQ DA D8 DJ H2 HJ S4 S5 ST SJ\n",
      "Revealed Bid Cards: P0:D3 P1:C3 P2:H7 P3:H2 \n",
      "Round Determination: Normal High Round (Sum > 13)\n",
      "Resulting Round Type (for scoring): High\n",
      "Trump suit: H\n",
      "Internal Target Bid values (for scoring logic): P0:3 P1:3 P2:7 P3:2 \n",
      "Highest bidder (determined trump): P2\n",
      "Trick #1\n",
      "Current trick (Leader P2): \n",
      "Trump break occurred (from previous tricks): No\n",
      "Tricks won: P0:0 P1:0 P2:0 P3:0 \n",
      "\n",
      "3. Trump Suit: H\n",
      "4. Ascend/Descend: Not applicable (bids did not sum to 13 or phase skipped).\n",
      "5. Final Round Type (for scoring): High (based on: Normal High Round (Sum > 13))\n",
      "\n",
      "--- 6. Trick Play Phase ---\n",
      "\n",
      "-- Trick 1 --\n",
      "  Tricks collected so far: P0:0, P1:0, P2:0, P3:0\n",
      "  Bid Cards: P0:D3 P1:C3 P2:H7 P3:H2\n",
      "  Broken: False\n",
      "  Trump Suit: H\n",
      "  Remaining Hands AT START of Trick 1:\n",
      "    P0: C2, C8, D3, D4, D5, D7, DQ, H3, H5, H9, HQ, S6, SQ\n",
      "    P1: C3, C7, CT, D2, D9, DK, H4, H6, H8, S3, S8, S9, SA\n",
      "    P2: C4, C6, CA, CK, D6, DT, H7, HA, HK, HT, S2, S7, SK\n",
      "    P3: C5, C9, CJ, CQ, D8, DA, DJ, H2, HJ, S4, S5, SJ, ST\n",
      "  Led by: P2\n",
      "\n",
      "  Player 2's turn.\n",
      "  Legal moves for P2: CA:0.22:4 DT:0.10:3 S2:0.11:1 C4:0.07:1 C6:0.02:3 S7:0.08:3 SK:0.27:3 CK:0.06:4 D6:0.07:3\n",
      "  A random choice: SK\n",
      "   -> P2 plays: SK\n",
      "\n",
      "  Player 3's turn.\n",
      "  Legal moves for P3: S4:0.32:1 S5:0.15:1 ST:0.24:1 SJ:0.29:1\n",
      "  A random choice: S5\n",
      "   -> P3 plays: S5\n",
      "\n",
      "  Player 0's turn.\n",
      "  Legal moves for P0: SQ:0.64:1 S6:0.36:1\n",
      "  A random choice: SQ\n",
      "   -> P0 plays: SQ\n",
      "\n",
      "  Player 1's turn.\n",
      "  Legal moves for P1: S3:0.13:1 S8:0.11:1 SA:0.62:4 S9:0.15:1\n",
      "  A random choice: SA\n",
      "   -> P1 plays: SA\n",
      "  Cards played this trick (P0-P3 order): SQ, SA, SK, S5\n",
      "\n",
      "-- Trick 2 --\n",
      "  Tricks collected so far: P0:0, P1:1, P2:0, P3:0\n",
      "  Bid Cards: P0:D3 P1:C3 P2:H7 P3:H2\n",
      "  Broken: False\n",
      "  Trump Suit: H\n",
      "  Remaining Hands AT START of Trick 2:\n",
      "    P0: C2, C8, D3, D4, D5, D7, DQ, H3, H5, H9, HQ, S6\n",
      "    P1: C3, C7, CT, D2, D9, DK, H4, H6, H8, S3, S8, S9\n",
      "    P2: C4, C6, CA, CK, D6, DT, H7, HA, HK, HT, S2, S7\n",
      "    P3: C5, C9, CJ, CQ, D8, DA, DJ, H2, HJ, S4, SJ, ST\n",
      "  Led by: P1\n",
      "\n",
      "  Player 1's turn.\n",
      "  Legal moves for P1: C3:0.07:1 D2:0.06:1 S3:0.03:3 S8:0.05:3 DK:0.29:3 C7:0.06:3 CT:0.35:3 D9:0.03:3 S9:0.06:3\n",
      "  A random choice: DK\n",
      "   -> P1 plays: DK\n",
      "\n",
      "  Player 2's turn.\n",
      "  Legal moves for P2: DT:0.61:1 D6:0.39:1\n",
      "  A random choice: DT\n",
      "   -> P2 plays: DT\n",
      "\n",
      "  Player 3's turn.\n",
      "  Legal moves for P3: DA:0.47:4 D8:0.34:1 DJ:0.18:1\n",
      "  A random choice: D8\n",
      "   -> P3 plays: D8\n",
      "\n",
      "  Player 0's turn.\n",
      "  Legal moves for P0: D5:0.14:1 D3:0.14:1 D7:0.12:1 D4:0.27:1 DQ:0.33:1\n",
      "  A random choice: D7\n",
      "Game quit by user during card play.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "\n",
    "from trump_interactive import interactive_trump_game\n",
    "\n",
    "state = interactive_trump_game(game, save_dir_nets=\"nfsp_nets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8904b638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 0 C3: 2 C4: 0 C5: 0 C6: 0 C7: 2 C8: 0 C9: 0 CT: 0 CJ: 2 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 0 D3: 0 D4: 0 D5: 0 D6: 0 D7: 0 D8: 0 D9: 2 DT: 2 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3:-1 H4: 2 H5: 2 H6: 0 H7: 0 H8: 2 H9: 2 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 0 S3: 2 S4: 2 S5: 0 S6: 0 S7: 2 S8: 0 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 0 SA: 0 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 0 C3:-1 C4: 0 C5: 0 C6: 0 C7: 2 C8: 0 C9: 0 CT: 0 CJ: 2 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 0 D3: 0 D4: 0 D5: 0 D6: 0 D7: 0 D8: 0 D9: 2 DT: 2 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3: 2 H4: 2 H5: 2 H6: 0 H7: 0 H8: 2 H9: 2 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 0 S3: 2 S4: 2 S5: 0 S6: 0 S7: 2 S8: 0 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 0 SA: 0 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 0 C3: 2 C4: 0 C5: 0 C6: 0 C7: 2 C8: 0 C9: 0 CT: 0 CJ: 2 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 0 D3: 0 D4: 0 D5: 0 D6: 0 D7: 0 D8: 0 D9: 2 DT: 2 DJ: 0 DQ: 0 DK: 2 DA: 0 \n",
      "H: H2: 0 H3: 2 H4:-1 H5: 2 H6: 0 H7: 0 H8: 2 H9: 2 HT: 0 HJ: 0 HQ: 2 HK: 0 HA: 0 \n",
      "S: S2: 0 S3: 2 S4: 2 S5: 0 S6: 0 S7: 2 S8: 0 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 0 SA: 0 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=1 Unknown=36, Had=0, Never Had=15\n",
      "Opponent 2: Has=1 Unknown=36, Had=0, Never Had=15\n",
      "Opponent 3: Has=1 Unknown=36, Had=0, Never Had=15\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "tensor = state.information_state_tensor(state.current_player())\n",
    "print_graveyards(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe21259f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 1.0, 2.0, 0.0, 0.0, 1.0, 0.0, 2.0, 1.0, 0.0, 0.0, 0.0, 3.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, -1.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, -1.0, 2.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 8.0, 8.0, 8.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb6bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 1 C3: 2 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9:-1 CT:-1 CJ: 2 CQ: 2 CK:-1 CA:-1 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 2 D6: 1 D7: 1 D8: 2 D9:-1 DT: 2 DJ: 2 DQ: 2 DK: 1 DA: 2 \n",
      "H: H2: 2 H3:-1 H4: 2 H5: 2 H6: 2 H7: 2 H8: 2 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 1 S4: 2 S5: 2 S6: 2 S7: 2 S8: 2 S9: 2 ST: 2 SJ: 1 SQ:-1 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 2 C3: 1 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9: 2 CT: 2 CJ:-1 CQ: 2 CK: 2 CA: 2 \n",
      "D: D2: 1 D3: 2 D4:-1 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT: 1 DJ: 2 DQ: 2 DK: 2 DA: 1 \n",
      "H: H2:-1 H3: 2 H4: 2 H5: 2 H6:-1 H7:-1 H8: 2 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 1 S3: 2 S4:-1 S5: 2 S6: 2 S7: 1 S8: 2 S9: 2 ST:-1 SJ: 2 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 2 C3: 2 C4:-1 C5:-1 C6: 1 C7: 2 C8: 2 C9: 2 CT: 2 CJ: 2 CQ:-1 CK: 2 CA: 2 \n",
      "D: D2: 2 D3: 1 D4: 2 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT: 2 DJ: 1 DQ: 1 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5:-1 H6: 2 H7: 2 H8: 2 H9: 2 HT: 2 HJ: 2 HQ:-1 HK: 2 HA:-1 \n",
      "S: S2: 2 S3: 2 S4: 2 S5: 1 S6: 2 S7: 2 S8: 2 S9: 2 ST: 2 SJ: 2 SQ: 2 SK: 1 SA:-1 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=7 Unknown=0, Had=6, Never Had=39\n",
      "Opponent 2: Has=7 Unknown=0, Had=6, Never Had=39\n",
      "Opponent 3: Has=7 Unknown=0, Had=6, Never Had=39\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import revelation_transformer\n",
    "\n",
    "tensor0 = state.information_state_tensor(0)\n",
    "tensor1 = state.information_state_tensor(1)\n",
    "tensor2 = state.information_state_tensor(2)\n",
    "tensor3 = state.information_state_tensor(3)\n",
    "all_tensors = [tensor0, tensor1, tensor2, tensor3]\n",
    "\n",
    "tensor_revealed = revelation_transformer(all_tensors, state.current_player())\n",
    "print_graveyards(tensor_revealed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d9331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38, 11, 37, 5, 40, 12, 9, 16, 50, 15, 32, 47, 14, 28, 10, 24, 43, 26, 17, 2, 19, 48, 4, 35, 18, 0, 3, 29, 25, 1, 30, 41, 34, 44, 7, 46, 27, 8, 20, 33, 51, 42, 39, 49, 21, 6, 22, 23, 45, 31, 36, 13, 40, 28, 27, 42, 13, 16, 24, 25, 23, 15, 19, 20, 21, 14, 18, 8, 39, 50, 48, 41, 51, 40, 43, 44, 22, 5, 17, 34, 45, 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 3246, in _complete\n",
      "    result = matcher(context)\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 2139, in magic_matcher\n",
      "    matches = self.magic_matches(text)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 2172, in magic_matches\n",
      "    global_matches = self.global_matches(bare_text)\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/mnt/s/py_repos/my-drl-gaming/venv_wsl/lib/python3.11/site-packages/IPython/core/completer.py\", line 1114, in global_matches\n",
      "    for word in lst:\n",
      "RuntimeError: dictionary changed size during iteration\n"
     ]
    }
   ],
   "source": [
    "print(state.history())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a4e77a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3♣ loses due to not following suit.\n",
      "6♣ loses due to not following suit.\n",
      "Q♦ loses due to not following suit.\n",
      "K♦ loses due to not following suit.\n",
      "A valid play where player plays 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: J♦ 8♥ 4♠ J♠ Q♠ A♠\n",
      "  P2: 3♦ 8♦ T♦ Q♥ 7♠ 9♠\n",
      "  P3: 5♦ 7♦ 2♥ 3♥ 9♥ K♠\n",
      "Current Trick:    P1:8♥ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 7♠ 9♠ Q♠ A♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ K♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ 4♠ J♠\n",
      "Current Trick:    P1:7♠ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with 5♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ Q♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ K♠\n",
      "Current Trick:    P1:4♠ P2:Q♥ P3:9♥\n",
      "A valid play where player plays T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ Q♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ K♠\n",
      "Current Trick:    P1:4♠ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ Q♠ K♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ 9♠ J♠\n",
      "Current Trick:    P1:K♠ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with T♠:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♦ 8♦ 4♠ 7♠ 9♠ K♠\n",
      "  P2: T♦ 2♥ 3♥ 8♥ Q♥ A♠\n",
      "  P3: 3♦ 5♦ J♦ 9♥ J♠ Q♠\n",
      "Current Trick:    P1:9♠ P2:Q♥ P3:9♥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trump_z3_bool_current_hand import analyze_hand_cards\n",
    "import numpy as np\n",
    "\n",
    "tensor = np.array(state.information_state_tensor(state.current_player()))\n",
    "analyze_hand_cards(tensor, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c1dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3♣ loses to 6♣\n",
      "5♣ loses to 6♣\n",
      "A valid play where player plays T♣:\n",
      "Opponent Current Hands:\n",
      "  P1: 4♣ 2♦ 5♦ 6♦ 7♦ 9♦ T♥ 2♠ 3♠ 4♠ T♠\n",
      "  P2: 9♣ 4♦ Q♦ K♦ 7♥ K♥ 5♠ 8♠ 9♠ Q♠ K♠\n",
      "  P3: 6♣ Q♣ A♣ 3♦ J♦ 5♥ 6♥ 8♥ J♥ J♠ A♠\n",
      "Current Trick:    P1:4♣ P2:9♣ P3:6♣\n",
      "A valid play where player plays K♣:\n",
      "Opponent Current Hands:\n",
      "  P1: 4♣ 2♦ 5♦ 6♦ 7♦ 9♦ T♥ 2♠ 3♠ 4♠ T♠\n",
      "  P2: 9♣ 4♦ Q♦ K♦ 7♥ K♥ 5♠ 8♠ 9♠ Q♠ K♠\n",
      "  P3: 6♣ Q♣ A♣ 3♦ J♦ 5♥ 6♥ 8♥ J♥ J♠ A♠\n",
      "Current Trick:    P1:4♣ P2:9♣ P3:6♣\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 4,\n",
       " -1,\n",
       " -1,\n",
       " 4,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from trump_utils import reveal_p0\n",
    "from trump_z3_bool_current_hand import analyze_hand_cards\n",
    "\n",
    "tensor_p1 = np.array(state.information_state_tensor(0))\n",
    "tensor_p2 = np.array(state.information_state_tensor(1))\n",
    "tensor_p3 = np.array(state.information_state_tensor(2))\n",
    "\n",
    "tensor_all = np.array([tensor, tensor_p1, tensor_p2, tensor_p3])\n",
    "\n",
    "tensor_revealed = reveal_p0(tensor_all)\n",
    "analyze_hand_cards(tensor_revealed, debug=True, max_timeout_ms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090101d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2:-1 C3: 2 C4: 2 C5: 2 C6: 2 C7:-1 C8: 2 C9: 2 CT: 2 CJ: 2 CQ: 2 CK:-1 CA: 2 \n",
      "D: D2:-1 D3: 2 D4: 2 D5: 1 D6: 2 D7: 2 D8: 2 D9: 2 DT: 2 DJ: 2 DQ:-1 DK: 2 DA:-1 \n",
      "H: H2: 2 H3:-1 H4: 2 H5: 2 H6: 2 H7:-1 H8:-1 H9: 2 HT: 2 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 2 S4: 2 S5:-1 S6: 2 S7:-1 S8: 2 S9: 2 ST: 2 SJ:-1 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 2 C3: 2 C4: 2 C5: 2 C6: 2 C7: 2 C8: 2 C9:-1 CT: 2 CJ:-1 CQ: 2 CK: 2 CA: 2 \n",
      "D: D2: 2 D3: 2 D4: 1 D5: 2 D6: 2 D7: 2 D8: 2 D9: 2 DT:-1 DJ: 2 DQ: 2 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5:-1 H6:-1 H7: 2 H8: 2 H9: 2 HT: 2 HJ:-1 HQ:-1 HK: 2 HA: 2 \n",
      "S: S2:-1 S3:-1 S4: 2 S5: 2 S6:-1 S7: 2 S8:-1 S9: 2 ST:-1 SJ: 2 SQ: 2 SK: 2 SA: 2 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 2 C3: 2 C4:-1 C5:-1 C6: 2 C7: 2 C8: 2 C9: 2 CT:-1 CJ: 2 CQ:-1 CK: 2 CA:-1 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 2 D6: 2 D7: 1 D8:-1 D9:-1 DT: 2 DJ: 2 DQ: 2 DK: 2 DA: 2 \n",
      "H: H2: 2 H3: 2 H4: 2 H5: 2 H6: 2 H7: 2 H8: 2 H9:-1 HT:-1 HJ: 2 HQ: 2 HK: 2 HA: 2 \n",
      "S: S2: 2 S3: 2 S4: 2 S5: 2 S6: 2 S7: 2 S8: 2 S9:-1 ST: 2 SJ: 2 SQ: 2 SK:-1 SA:-1 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=12 Unknown=0, Had=1, Never Had=39\n",
      "Opponent 2: Has=12 Unknown=0, Had=1, Never Had=39\n",
      "Opponent 3: Has=12 Unknown=0, Had=1, Never Had=39\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "print_graveyards(tensor_revealed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39146a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q♣ loses due to not following suit.\n",
      "K♣ loses due to not following suit.\n",
      "3♠ loses due to not following suit.\n",
      "6♠ loses due to not following suit.\n",
      "A valid play where player plays 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: J♣ J♦ Q♦ 2♥ 3♥ 8♥\n",
      "  P2: 5♣ 7♣ T♣ 4♦ 7♦ Q♥\n",
      "  P3: 3♣ 8♣ 9♦ K♦ A♦ 9♥\n",
      "Current Trick:    P1:3♥ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 7♦ Q♦ K♦ A♦\n",
      "  P2: 5♣ 9♦ J♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ T♣ J♣ 4♦ 2♥ 9♥\n",
      "Current Trick:    P1:7♦ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with 5♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ 7♦ K♦ A♦\n",
      "  P2: 9♦ J♦ Q♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:4♦ P2:Q♥ P3:9♥\n",
      "A valid play where player plays T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ 7♦ K♦ A♦\n",
      "  P2: 9♦ J♦ Q♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:4♦ P2:Q♥ P3:9♥\n",
      "A scenario where player loses with T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 7♣ 8♣ 4♦ Q♦ K♦ A♦\n",
      "  P2: 7♦ 9♦ J♦ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ T♣ J♣ 2♥ 9♥\n",
      "Current Trick:    P1:Q♦ P2:Q♥ P3:9♥\n",
      "A scenario where player wins with T♦:\n",
      "Opponent Current Hands:\n",
      "  P1: 8♣ 4♦ 7♦ Q♦ K♦ A♦\n",
      "  P2: 9♦ J♦ 2♥ 3♥ 8♥ Q♥\n",
      "  P3: 3♣ 5♣ 7♣ T♣ J♣ 9♥\n",
      "Current Trick:    P1:7♦ P2:Q♥ P3:9♥\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 3,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1,\n",
       " -1]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trump_utils import data_augmentor\n",
    "import jax.numpy as jnp\n",
    "\n",
    "tensor_permuted = data_augmentor(jnp.asarray(tensor), jax.random.PRNGKey(22))\n",
    "analyze_hand_cards(tensor_permuted, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005da7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graveyards (Opponent Card Knowledge):\n",
      "Values: -1=has, 0=unknown, 1=had, 2=never had\n",
      "\n",
      "Opponent 1:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 2 D4: 2 D5: 0 D6: 1 D7: 2 D8: 0 D9: 0 DT: 2 DJ: 2 DQ: 0 DK: 0 DA: 1 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5:-1 S6: 2 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Opponent 2:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 2 D4: 1 D5: 0 D6: 2 D7: 2 D8: 0 D9: 0 DT: 1 DJ: 2 DQ: 0 DK: 0 DA: 2 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5: 2 S6: 2 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Opponent 3:\n",
      "C: C2: 0 C3: 2 C4: 2 C5: 2 C6: 2 C7: 0 C8: 0 C9: 0 CT: 0 CJ: 0 CQ: 2 CK: 0 CA: 0 \n",
      "D: D2: 2 D3: 1 D4: 2 D5: 0 D6: 2 D7: 2 D8: 0 D9: 0 DT: 2 DJ: 1 DQ: 0 DK: 0 DA: 2 \n",
      "H: H2: 0 H3: 0 H4: 0 H5: 0 H6: 2 H7: 0 H8: 2 H9: 0 HT: 0 HJ: 0 HQ: 0 HK: 0 HA: 2 \n",
      "S: S2: 2 S3: 0 S4: 0 S5: 2 S6:-1 S7: 0 S8: 2 S9: 0 ST: 0 SJ: 0 SQ: 0 SK: 2 SA: 0 \n",
      "\n",
      "Summary:\n",
      "Opponent 1: Has=1 Unknown=31, Had=2, Never Had=18\n",
      "Opponent 2: Has=0 Unknown=31, Had=2, Never Had=19\n",
      "Opponent 3: Has=1 Unknown=31, Had=2, Never Had=18\n"
     ]
    }
   ],
   "source": [
    "from trump_utils import print_graveyards\n",
    "\n",
    "tensor = state.information_state_tensor(state.current_player())\n",
    "print_graveyards(np.array(tensor_permuted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7021b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 3.0, 1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 1.0, 0.0, 0.0, 3.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, -1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 2.0, 2.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 2.0, 2.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 2.0, -1.0, 0.0, 0.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 0.0, 6.0, 8.0, 6.0, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93259c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "588"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f5df7",
   "metadata": {},
   "source": [
    "## Players from different iterations playing against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48551e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax\n",
    "jax.config.update('jax_platform_name', 'cpu')\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "import flax.linen as nn\n",
    "import pyspiel\n",
    "from typing import List, Dict, Any, Callable\n",
    "from collections import defaultdict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def load_params(save_dir_nets, player, phase, iteration):\n",
    "    \"\"\"Load policy parameters for a player, phase, and iteration.\"\"\"\n",
    "    pi_params_path = os.path.join(save_dir_nets, f\"player{player}\", f\"phase{phase}\", \"pi_data\", f\"pi_params_iter{iteration}.msgpack\")\n",
    "    if not os.path.exists(pi_params_path):\n",
    "        raise FileNotFoundError(f\"Parameter file not found: {pi_params_path}\")\n",
    "    with open(pi_params_path, 'rb') as f:\n",
    "        state_dict = flax.serialization.from_bytes(None, f.read())\n",
    "    if 'params' not in state_dict:\n",
    "        raise KeyError(f\"'params' key not found in loaded state_dict from {pi_params_path}\")\n",
    "    return state_dict['params']\n",
    "\n",
    "def evaluate_players(\n",
    "    game_name: str,\n",
    "    save_dir_nets: str,\n",
    "    iterations_per_player: List[int],\n",
    "    num_eval_games: int,\n",
    "    pi_models: List[nn.Module],\n",
    "    info_state_tensor_transformers: List[Callable[[jax.Array], jax.Array]],\n",
    "    action_transformers: List[Callable[[jax.Array], jax.Array]],\n",
    "    phase_classifier_fn: Callable[[jax.Array], int],\n",
    "    uniform: bool = False,\n",
    "    seed: int = 42\n",
    ") -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Evaluate players by having them play against each other.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Name of the game (e.g., 'kuhn_poker')\n",
    "        save_dir_nets: Directory where network parameters are saved\n",
    "        iterations_per_player: List of iteration numbers for each player [player0_iter, player1_iter, ...]\n",
    "        num_eval_games: Number of games to play for evaluation\n",
    "        pi_models: List of policy network models for each phase\n",
    "        info_state_tensor_transformers: List of transformers for each phase\n",
    "        action_transformers: List of action transformers for each phase\n",
    "        phase_classifier_fn: Function to classify game phase from info state\n",
    "        seed: Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping player_id to average score across all games\n",
    "    \"\"\"\n",
    "    # Initialize game and random state\n",
    "    game = pyspiel.load_game(game_name)\n",
    "    num_players = game.num_players()\n",
    "    global_num_actions = game.num_distinct_actions()\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    num_phases = len(pi_models)\n",
    "    \n",
    "    if len(iterations_per_player) != num_players:\n",
    "        raise ValueError(f\"Number of iterations ({len(iterations_per_player)}) must match number of players ({num_players})\")\n",
    "    \n",
    "    # Load all player parameters for all phases\n",
    "    player_params = []\n",
    "    for player in range(num_players):\n",
    "        player_phase_params = []\n",
    "        for phase in range(num_phases):\n",
    "            params = load_params(save_dir_nets, player if not uniform else 0, phase, iterations_per_player[player])\n",
    "            player_phase_params.append(params)\n",
    "        player_params.append(player_phase_params)\n",
    "    \n",
    "    # Setup JIT compiled inference functions for all players and phases\n",
    "    jitted_inference_pi = []\n",
    "    for phase in range(num_phases):\n",
    "        phase_inferences = []\n",
    "        for player in range(num_players if not uniform else 1):\n",
    "            inference_fn = _get_jitted_avg_policy(pi_models[phase], action_transformers[phase])\n",
    "            phase_inferences.append(inference_fn)\n",
    "        jitted_inference_pi.append(phase_inferences)\n",
    "    \n",
    "    # Play evaluation games\n",
    "    total_scores = defaultdict(float)\n",
    "    \n",
    "    for _game_round in tqdm(range(num_eval_games)):\n",
    "        state = game.new_initial_state()\n",
    "        \n",
    "        while not state.is_terminal():\n",
    "            if state.is_chance_node():\n",
    "                # Handle chance nodes\n",
    "                chance_outcome_actions, chance_outcome_probs = zip(*state.chance_outcomes())\n",
    "                chance_outcome_probs_np = np.array(chance_outcome_probs, dtype=np.float64)\n",
    "                chance_outcome_probs_np /= np.sum(chance_outcome_probs_np)\n",
    "                sampled_action = np.random.choice(chance_outcome_actions, p=chance_outcome_probs_np)\n",
    "                state = state.child(sampled_action)\n",
    "            else:\n",
    "                # Handle player decision nodes\n",
    "                active_player = state.current_player()\n",
    "                \n",
    "                # Get info state and determine phase\n",
    "                full_info_state_np = np.array(state.information_state_tensor(active_player), dtype=np.float32)\n",
    "                phase = phase_classifier_fn(full_info_state_np)\n",
    "                \n",
    "                # Transform info state for this phase\n",
    "                info_state_transformed_np = info_state_tensor_transformers[phase](full_info_state_np)\n",
    "                legal_actions_mask_global_np = np.array(state.legal_actions_mask(active_player), dtype=bool)\n",
    "                \n",
    "                # Add batch dimensions for network inference\n",
    "                info_state_transformed_np = jnp.expand_dims(info_state_transformed_np, axis=0)\n",
    "                legal_actions_mask_global_np = jnp.expand_dims(legal_actions_mask_global_np, axis=0)\n",
    "                \n",
    "                # Get policy probabilities from the network\n",
    "                policy_probs_global = jitted_inference_pi[phase][active_player if not uniform else 0](\n",
    "                    player_params[active_player if not uniform else 0][phase],\n",
    "                    info_state_transformed_np,\n",
    "                    legal_actions_mask_global_np\n",
    "                )\n",
    "                \n",
    "                # Convert to numpy and ensure proper normalization\n",
    "                policy_probs_np_global = np.array(policy_probs_global)\n",
    "                policy_probs_np_global /= np.sum(policy_probs_np_global)\n",
    "                \n",
    "                # Sample action according to policy\n",
    "                sampled_action = np.random.choice(global_num_actions, p=policy_probs_np_global)\n",
    "                state = state.child(sampled_action)\n",
    "        \n",
    "        # Return final scores/returns for all players\n",
    "        returns = state.returns()\n",
    "        game_scores = {player: returns[player] for player in range(len(returns))}\n",
    "        \n",
    "        # Accumulate scores\n",
    "        for player, score in game_scores.items():\n",
    "            total_scores[player] += score\n",
    "    \n",
    "    # Calculate average scores\n",
    "    average_scores = {player_idx: total_scores[player_idx] / num_eval_games \n",
    "                     for player_idx in range(num_players)}\n",
    "    \n",
    "    return average_scores\n",
    "\n",
    "def _get_jitted_avg_policy(pi_model_instance, action_transformer_fn):\n",
    "    \"\"\"Create JIT compiled policy inference function.\"\"\"\n",
    "    @jax.jit\n",
    "    def get_policy(params_avg_policy: Any, info_state_transformed: jax.Array, legal_actions_mask_global: jax.Array):\n",
    "        masked_logits_net_output = pi_model_instance.apply(\n",
    "            {'params': params_avg_policy}, info_state_transformed, legal_actions_mask_global\n",
    "        )\n",
    "        masked_logits_global = action_transformer_fn(masked_logits_net_output)\n",
    "        avg_policy_probs_global = jax.nn.softmax(masked_logits_global, axis=-1)\n",
    "        return jnp.squeeze(avg_policy_probs_global, axis=0)\n",
    "    return get_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9863927a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "808b163ff72b400fb5fac983461b596f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 55\u001b[0m\n\u001b[1;32m     47\u001b[0m info_state_tensor_transformers \u001b[38;5;241m=\u001b[39m [bid_transformer, ad_transformer, play_transformer_pi]\n\u001b[1;32m     49\u001b[0m pi_models \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     50\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpBiddingPolicyNet()),  \u001b[38;5;66;03m# Phase 0\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpAD_PolicyNet()),      \u001b[38;5;66;03m# Phase 1\u001b[39;00m\n\u001b[1;32m     52\u001b[0m     PolicyNetworkWrapper(phase_net\u001b[38;5;241m=\u001b[39mTrumpPlayPolicyNet())      \u001b[38;5;66;03m# Phase 2\u001b[39;00m\n\u001b[1;32m     53\u001b[0m ]\n\u001b[0;32m---> 55\u001b[0m \u001b[43mevaluate_players\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgame_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrump\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_dir_nets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcfvfp_nets_z3-q-only\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43miterations_per_player\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m300\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m700\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muniform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_eval_games\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpi_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpi_models\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo_state_tensor_transformers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfo_state_tensor_transformers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[43maction_transformers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_transformers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphase_classifier_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrump_phase_classifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\n\u001b[1;32m     66\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 124\u001b[0m, in \u001b[0;36mevaluate_players\u001b[0;34m(game_name, save_dir_nets, iterations_per_player, num_eval_games, pi_models, info_state_tensor_transformers, action_transformers, phase_classifier_fn, uniform, seed)\u001b[0m\n\u001b[1;32m    121\u001b[0m         policy_probs_np_global \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(policy_probs_np_global)\n\u001b[1;32m    123\u001b[0m         \u001b[38;5;66;03m# Sample action according to policy\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m         sampled_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(global_num_actions, p\u001b[38;5;241m=\u001b[39mpolicy_probs_np_global)\n\u001b[1;32m    125\u001b[0m         state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mchild(sampled_action)\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Return final scores/returns for all players\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from trump_utils import (\n",
    "    PolicyNetworkWrapper,\n",
    "    TrumpBiddingPolicyNet, TrumpAD_PolicyNet, TrumpPlayPolicyNet,\n",
    "    action_transformers, #info_state_tensor_transformers,\n",
    "    trump_phase_classifier\n",
    ")\n",
    "\n",
    "# --- Tensor Specification, Slicing Helper, and Phase Detection Indices ---\n",
    "TENSOR_COMPONENT_SPEC = [\n",
    "    (\"Hand\", 52), \n",
    "    (\"BidCards\", 4 * 5),       # 20\n",
    "    (\"TrumpSuit\", 4),          \n",
    "    (\"RoundBidStatus\", 1),     \n",
    "    (\"History\", 13 * (4 + 4 * 5)),   # 312\n",
    "    (\"OpponentGraveyard\", 3 * 52), # 156\n",
    "    (\"ANTC\", 4),               \n",
    "    (\"BreakOccurred\", 1),      \n",
    "    (\"CurrentTrickCards\", 4 * 5), # 20\n",
    "    (\"CurrentTrickLeader\", 4), \n",
    "    (\"CurrentTrickTrumpUncertainty\", 13),    # NEW ITEM #11\n",
    "    (\"CurrentTrickNumber\", 1)  \n",
    "]\n",
    "\n",
    "GLOBAL_NUM_ACTIONS = 52\n",
    "\n",
    "_tensor_component_slices: Dict[str, slice] = {}\n",
    "_current_offset = 0\n",
    "for _name, _size in TENSOR_COMPONENT_SPEC:\n",
    "    _tensor_component_slices[_name] = slice(_current_offset, _current_offset + _size)\n",
    "    _current_offset += _size\n",
    "\n",
    "# Define indices for phase classification using the slices\n",
    "INDEX_ROUND_BID_STATUS = _tensor_component_slices[\"RoundBidStatus\"].start\n",
    "INDEX_FIRST_BID_CARD_FEATURE = _tensor_component_slices[\"BidCards\"].start\n",
    "INDEX_HAND_START = _tensor_component_slices[\"Hand\"].start\n",
    "INDEX_HAND_END = _tensor_component_slices[\"Hand\"].stop\n",
    "INDEX_BID_CARDS_END = _tensor_component_slices[\"BidCards\"].stop\n",
    "\n",
    "def bid_transformer(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor[..., INDEX_HAND_START:INDEX_HAND_END]\n",
    "\n",
    "def ad_transformer(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor[..., INDEX_HAND_START:INDEX_BID_CARDS_END]\n",
    "\n",
    "def play_transformer_pi(infostate_tensor: jax.Array) -> jax.Array:\n",
    "    return infostate_tensor\n",
    "info_state_tensor_transformers = [bid_transformer, ad_transformer, play_transformer_pi]\n",
    "\n",
    "pi_models = [\n",
    "    PolicyNetworkWrapper(phase_net=TrumpBiddingPolicyNet()),  # Phase 0\n",
    "    PolicyNetworkWrapper(phase_net=TrumpAD_PolicyNet()),      # Phase 1\n",
    "    PolicyNetworkWrapper(phase_net=TrumpPlayPolicyNet())      # Phase 2\n",
    "]\n",
    "\n",
    "evaluate_players(\n",
    "    game_name=\"trump\",\n",
    "    save_dir_nets=\"nfsp_nets_z3-q-only\",\n",
    "    iterations_per_player=[300, 300, 300, 700],\n",
    "    uniform=True,\n",
    "    num_eval_games=1000,\n",
    "    pi_models=pi_models,\n",
    "    info_state_tensor_transformers=info_state_tensor_transformers,\n",
    "    action_transformers=action_transformers,\n",
    "    phase_classifier_fn=trump_phase_classifier,\n",
    "    seed=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998f469c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_wsl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
